{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Starting Words\n\nThis notebook is targeted for people who are new to the data analysis. The notebook explains the process of data modeling in the simplest way, including the detail on how I approach the process of problem solving.\n\n### Data Analysis Process\nI follow the following steps fod data analysis:\n1. Define the problem\n2. Analyze and prepare the data\n3. Develop and evaluate the models\n4. Improve results\n5. Present results","metadata":{}},{"cell_type":"markdown","source":"## Define the problem\nThe insurance company needs a program that will predict the insurance charges (premium) to be levied to a new client. The task is to develop a model that will provide a charge amount whenever certain input regarding the new client is given to the model. The company has data on existing clients on age, sex, bmi, smoking habit, and others. Based on these variables, it is expected to predict the charges to a new client with utmost accuracy. \n\nCalculating an insurance charge is a complex and time-consuming process. Developing a model to predict the insurance charge will help the company to avoid the burden of manual calculation and reduce the risk of human error. Once the model is developed, the manual tasks of calculation are omitted from the system, which ultimately enhances efficiency, work speed, and reduction in operating resource cost. \n\nIn the given problem, the target variable is numeric in nature and we will be developing regression models (both linear and non-linear). The accuracy of the models will be measured in terms of R square.  ","metadata":{}},{"cell_type":"markdown","source":"## Analyze and Prepare the data\nAnalyzing the data includes the descriptive analysis and visualization. We will not be going for EDA (Exploratory Data Analysis), and focus on basic analysis only. Let us begin by importing necessary libraries and data.","metadata":{}},{"cell_type":"code","source":"### Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n### Library to encode categorical variable\nfrom sklearn.preprocessing import LabelEncoder\n\n### Linear Regression model libraries\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\n\n### Import the data as a panda dataframe\ndf = pd.read_csv('../input/data-visualizatiion/insurance.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.321907Z","iopub.execute_input":"2022-07-24T06:55:50.322281Z","iopub.status.idle":"2022-07-24T06:55:50.338121Z","shell.execute_reply.started":"2022-07-24T06:55:50.322252Z","shell.execute_reply":"2022-07-24T06:55:50.337303Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"### Data summary\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.377567Z","iopub.execute_input":"2022-07-24T06:55:50.378289Z","iopub.status.idle":"2022-07-24T06:55:50.385232Z","shell.execute_reply.started":"2022-07-24T06:55:50.378242Z","shell.execute_reply":"2022-07-24T06:55:50.384065Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains 1338 entities (rows) and 7 attributes (columns).","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.464066Z","iopub.execute_input":"2022-07-24T06:55:50.465047Z","iopub.status.idle":"2022-07-24T06:55:50.481563Z","shell.execute_reply.started":"2022-07-24T06:55:50.465011Z","shell.execute_reply":"2022-07-24T06:55:50.480532Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"We can observe the first 10 entires of the dataset. Here, age, bmi. children and charges are numeric in nature, and others are categorical. In real case, we will have large number of attributes (more than 50) and it is not possible to seperate the nature of variable by visual observation. Hence, we have a separate function to check the nature of variable. ","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.517643Z","iopub.execute_input":"2022-07-24T06:55:50.518237Z","iopub.status.idle":"2022-07-24T06:55:50.525477Z","shell.execute_reply.started":"2022-07-24T06:55:50.518206Z","shell.execute_reply":"2022-07-24T06:55:50.524604Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Here, we can check the nature of attributes in detail. The number of children should be categorical also, but the program identified it as numeric due to the presence of number. Hence, we will change the data type to object.","metadata":{}},{"cell_type":"code","source":"df.children = df.children.astype(object)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.527312Z","iopub.execute_input":"2022-07-24T06:55:50.527882Z","iopub.status.idle":"2022-07-24T06:55:50.534994Z","shell.execute_reply.started":"2022-07-24T06:55:50.527850Z","shell.execute_reply":"2022-07-24T06:55:50.534067Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"One of the most important aspect of data preparation is checking the missing values and NAN in the dataset and making suitable adjustments.","metadata":{}},{"cell_type":"code","source":"### Check whether there is any missing values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.586938Z","iopub.execute_input":"2022-07-24T06:55:50.587620Z","iopub.status.idle":"2022-07-24T06:55:50.599731Z","shell.execute_reply.started":"2022-07-24T06:55:50.587585Z","shell.execute_reply":"2022-07-24T06:55:50.598504Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Here, the number of missing values for each column is shown. Since, all the columns have zero missing values, we do not need any adjustments.\nNow, we continue with the descriptive analysis for numeric attributes.","metadata":{}},{"cell_type":"code","source":"### Descriptive Statistics\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.629007Z","iopub.execute_input":"2022-07-24T06:55:50.629737Z","iopub.status.idle":"2022-07-24T06:55:50.652181Z","shell.execute_reply.started":"2022-07-24T06:55:50.629668Z","shell.execute_reply":"2022-07-24T06:55:50.651050Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"The descriptive analysis provides detail on 7 parameters as shown in the table. For age and bmi attributes, the mean value and median values are similar in nature, hence, we can expect less skweness. However, the median of the charges is lower than median signaling the presence of skewness. Moreover, the maximum value of charges is well-above the mean and is the likely cause of the skewness. Hence, we can treat the higher charges as an outlier and remove from data for analysis. For this, let us check the data with charges above 45,000.","metadata":{}},{"cell_type":"code","source":"df[df['charges'] > 45000]['charges'].count()/df['charges'].count()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.654418Z","iopub.execute_input":"2022-07-24T06:55:50.655169Z","iopub.status.idle":"2022-07-24T06:55:50.664341Z","shell.execute_reply.started":"2022-07-24T06:55:50.655127Z","shell.execute_reply":"2022-07-24T06:55:50.663486Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"About 3% of data have charges above the 45,000. We consider them as an outlier and remove from the dataset.","metadata":{}},{"cell_type":"code","source":"### Remove Outliers\ndf = df[df['charges'] <= 45000]","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.686782Z","iopub.execute_input":"2022-07-24T06:55:50.687624Z","iopub.status.idle":"2022-07-24T06:55:50.695218Z","shell.execute_reply.started":"2022-07-24T06:55:50.687591Z","shell.execute_reply":"2022-07-24T06:55:50.694341Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization\nWe will continue the analysis of data through visualization.","metadata":{}},{"cell_type":"code","source":"### Numerical variable visualization\nsns.pairplot(df)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:50.696997Z","iopub.execute_input":"2022-07-24T06:55:50.697559Z","iopub.status.idle":"2022-07-24T06:55:54.215699Z","shell.execute_reply.started":"2022-07-24T06:55:50.697528Z","shell.execute_reply":"2022-07-24T06:55:54.214621Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"While analyzing the numeric data, we should look for the correlation between two attributes, with special focus on target varible, that is, charge. Here, charges and age attributes show some kind of linear growth pattern signalling the possibility of correlation. BMI and charges plot has no significant pattern and hence, correlation is less likely.","metadata":{}},{"cell_type":"code","source":"### Numerical and categorical variable visualization\nsns.pairplot(df, hue = 'sex')\nsns.pairplot(df, hue = 'region')\nsns.pairplot(df, hue = 'children')\nsns.pairplot(df, hue = 'smoker')","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:55:54.217568Z","iopub.execute_input":"2022-07-24T06:55:54.217897Z","iopub.status.idle":"2022-07-24T06:56:13.619019Z","shell.execute_reply.started":"2022-07-24T06:55:54.217867Z","shell.execute_reply":"2022-07-24T06:56:13.617768Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"The graph with smoker attribute has significant patterns. In general, the charge amount is lower for people who do not smoke. It is likely that smoking habit affects the insurance charge. Other categorical variables are likely not to have significant impact on charge.","metadata":{}},{"cell_type":"markdown","source":"## Develop and Evaluate the model\nNow we move to the next steps where we develop the models and evaluate their performance. Here, we need to understand three concepts: Machine Language Alogrithm, Resampling and Performance Metrics. \n\n### Machine Learning Alogrithm\nA machine learning algorithm is the method by used to conduct the specified task, generally predicting output values from given input data. In this problem solving, we are applying regression method (both linear and non-linear) from supervised machine learning alogrithms and selecting the best performing.\n\nThe linear machine learning algorithms are as follow:\n1. Linear Regression.\n2. Ridge Regression.\n3. LASSO Linear Regression.\n4. Elastic Net Regression.\n\nThe nonlinear machine learning algorithms are:\n1. k-Nearest Neighbors.\n2. Classification and Regression Trees.\n3. Support Vector Machines.\n\n### Resampling\nIn order to evaluate the performance of a machine learning alogrithm, we use train-test method, where available data is divided into train set and test set in certain ratio (generally 80:20). We build the model based on train dataset and make prediction on test dataset to measure the accuracy by comparing to actual target value in test dataset. \n\nThe problem with this method is that we only have a single estimate, with little idea of the variability or uncertainty in the estimate. We can address this issue by estimating the performance parameter multiple times from our data sample. In other words, we change the sample data in train and test datasets multiple times and measure the accuracy of the model.\n\nThe different techniques that we can use for resampling are:\n* Train and Test Sets.\n* k-fold Cross Validation.\n* Leave One Out Cross Validation.\n* Repeated Random Test-Train Splits.\n\n### Performance Metrics\nPerformance metrics are the indicators used to evaluate the performance of machine learning algorithms. The most common metrics for evaluating predictions on regression machine learning problems are:\n* Mean Absolute Error.\n* Mean Squared Error.\n* R square\n\n### Work Model\nTo better understand the ML alogrithm and Resampling, we will develop a single linear regression model based on train-test dataset and then, develop multiple models based on cross-validation resampling. We will use R square as performance metric.\n\nFirst, we will convert the object variable into categorical and then apply labelencoder to convert them into numericals.","metadata":{}},{"cell_type":"code","source":"##Converting objects labels into categorical\ndf[['sex', 'smoker', 'region', 'children']] = df[['sex', 'smoker', 'region', 'children']].astype('category')\ndf.dtypes\n\n##Converting category labels into numerical using LabelEncoder\nlabel = LabelEncoder()\n\nlabel.fit(df.sex.drop_duplicates())\ndf.sex = label.transform(df.sex)\n\nlabel.fit(df.smoker.drop_duplicates())\ndf.smoker = label.transform(df.smoker)\n\nlabel.fit(df.region.drop_duplicates())\ndf.region = label.transform(df.region)\n\nlabel.fit(df.children.drop_duplicates())\ndf.children = label.transform(df.children)\n\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:13.620684Z","iopub.execute_input":"2022-07-24T06:56:13.621049Z","iopub.status.idle":"2022-07-24T06:56:13.645935Z","shell.execute_reply.started":"2022-07-24T06:56:13.621016Z","shell.execute_reply":"2022-07-24T06:56:13.644898Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"### Segregrating dataframe in independent and target variable\nx = df.drop(['charges'], axis = 1)  #Independent Variables\ny = df['charges']                   #Target Variables","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:13.649303Z","iopub.execute_input":"2022-07-24T06:56:13.650333Z","iopub.status.idle":"2022-07-24T06:56:13.657385Z","shell.execute_reply.started":"2022-07-24T06:56:13.650283Z","shell.execute_reply":"2022-07-24T06:56:13.656393Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Here, we have all independent variables in the dataframe x and target variable is in y.","metadata":{}},{"cell_type":"code","source":"### Linear regression with train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nr_sq = model.score(x, y)\nprint(f\"coefficient of determination: {r_sq}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:13.659035Z","iopub.execute_input":"2022-07-24T06:56:13.659447Z","iopub.status.idle":"2022-07-24T06:56:13.679086Z","shell.execute_reply.started":"2022-07-24T06:56:13.659405Z","shell.execute_reply":"2022-07-24T06:56:13.677965Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"We first split the dataset into train and test dataset in the ration 80:20. We used linear regression as our model and fitted the train dataset. The coefficient of determination or R square is obtained as printed.","metadata":{}},{"cell_type":"markdown","source":"### Evaluating multiple models\nNow, we will develop different regression models and evaluate their performance. For this, we create a list of models with appropriate ML alogrithm. The result capture average R square for each model based on the cross validation method.","metadata":{}},{"cell_type":"code","source":"### Prepare a list of models\nmodels = []\nmodels.append(('Linear', LinearRegression()))\nmodels.append(('Ridge', Ridge(alpha = 0.5)))\nmodels.append(('Lasso', Lasso(alpha=0.2, fit_intercept=True, normalize=False, precompute=False, max_iter=1000,\n              tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')))\nmodels.append(('Elastic', ElasticNet()))\nmodels.append(('Kneighbour', KNeighborsRegressor()))\nmodels.append(('CART',  DecisionTreeRegressor()))\nmodels.append(('SVR',  SVR()))","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:13.680668Z","iopub.execute_input":"2022-07-24T06:56:13.681443Z","iopub.status.idle":"2022-07-24T06:56:13.690194Z","shell.execute_reply.started":"2022-07-24T06:56:13.681393Z","shell.execute_reply":"2022-07-24T06:56:13.688994Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"### Evaluate each model in terms of R square\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n    cv_results = cross_val_score(model, x, y)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:13.691787Z","iopub.execute_input":"2022-07-24T06:56:13.692739Z","iopub.status.idle":"2022-07-24T06:56:14.294279Z","shell.execute_reply.started":"2022-07-24T06:56:13.692668Z","shell.execute_reply":"2022-07-24T06:56:14.293089Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Hence, Linear, Ridge and Lasso regression method has smiliar accuracy and are the best method for the current dataset. Support Vector Regression (SVR) has the worst performance.","metadata":{}},{"cell_type":"code","source":"# Visual representation of model comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-24T06:56:14.295764Z","iopub.execute_input":"2022-07-24T06:56:14.296114Z","iopub.status.idle":"2022-07-24T06:56:14.526832Z","shell.execute_reply.started":"2022-07-24T06:56:14.296082Z","shell.execute_reply":"2022-07-24T06:56:14.525772Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Improve Results\nWe have further option to improve the result with Ensembles and Alogrithm Tuning. However, the current dataset is not that complex and the result obtained is satisfactory. Hence, futher work of improving the result is not required.","metadata":{}},{"cell_type":"markdown","source":"## Present Results\nThe result can be presented in different forms - sildes, report and other methods. For my work, this notebook is used as a document to present the result.","metadata":{}}]}